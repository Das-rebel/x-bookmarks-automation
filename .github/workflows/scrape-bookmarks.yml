name: Daily X.com Bookmark Scraper

on:
  schedule:
    - cron: '0 4 * * *'  # 9 AM UTC daily
  workflow_dispatch:  # Allow manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
      with:
        # Force fetch all history to ensure we get all changes
        fetch-depth: 0

    - name: Setup Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
        cache: 'npm'

    - name: Install dependencies
      run: |
        # Ensure we're using the latest version of the script
        git pull origin main
        # Verify script version
        echo "Script version from file:" $(grep -m 1 'Script started - Version' github-actions-scraper.js)
        # Install dependencies
        npm install
        npm list puppeteer

    - name: Run scraper
      env:
        X_USERNAME: ${{ secrets.X_USERNAME }}
        X_PASSWORD: ${{ secrets.X_PASSWORD }}
        N8N_WEBHOOK_URL: ${{ secrets.N8N_WEBHOOK_URL }}
        BROWSERLESS_TOKEN: ${{ secrets.BROWSERLESS_TOKEN }}
      run: |
        echo "Starting script execution..."
        node github-actions-scraper.js || (
          echo "Script failed with exit code $?"
          exit 1
        )
        echo "Script completed successfully"
    
    - name: Upload bookmarks
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: bookmarks
        path: bookmarks.json
        retention-days: 1
