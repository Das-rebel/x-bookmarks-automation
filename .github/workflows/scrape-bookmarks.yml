name: Daily X.com Bookmark Scraper

on:
  schedule:
    - cron: '0 4 * * *'  # 9 AM UTC daily
  workflow_dispatch:  # Allow manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
      with:
        # Force fetch all history to ensure we get all changes
        fetch-depth: 0
        # Force a fresh clone to avoid any caching issues
        clean: true

    - name: Setup Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
        cache: 'npm'

    - name: Install dependencies
      run: |
        # Print current directory and files for debugging
        echo "Current directory: $(pwd)"
        echo "Files in directory:"
        ls -la
        
        # Verify script version
        echo "=== Script content (first 20 lines) ==="
        head -n 20 github-actions-scraper.js
        echo "=== End of script content ==="
        
        # Install dependencies using package-lock.json
        echo "Installing dependencies using package-lock.json..."
        npm ci
        echo "=== Installed packages ==="
        npm list
        
        # Verify puppeteer version
        echo "=== Puppeteer version ==="
        npm list puppeteer

    - name: Run scraper
      env:
        X_USERNAME: ${{ secrets.X_USERNAME }}
        X_PASSWORD: ${{ secrets.X_PASSWORD }}
        N8N_WEBHOOK_URL: ${{ secrets.N8N_WEBHOOK_URL }}
        BROWSERLESS_TOKEN: ${{ secrets.BROWSERLESS_TOKEN }}
      run: |
        echo "Starting script execution..."
        node github-actions-scraper.js || (
          echo "Script failed with exit code $?"
          exit 1
        )
        echo "Script completed successfully"
    
    - name: Upload bookmarks
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: bookmarks
        path: bookmarks.json
        retention-days: 1
